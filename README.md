# Personalized Resume Assistant API

A modern Retrieval-Augmented Generation (RAG) API that helps job seekers tailor their resumes and applications. Upload resumes, job descriptions, or related documents, then ask targeted questions to receive context-aware answers generated by a powerful language model.

The system is built on FastAPI, integrates Groq for ultra-fast inference with automatic Gemini fallback for reliability, and implements a complete document-to-answer pipeline.

## Table of Contents

- [Features](#features)
- [Architecture](#architecture)
- [Getting Started](#getting-started)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
  - [Configuration](#configuration)
- [Usage](#usage)
  - [Running the Server](#running-the-server)
  - [API Endpoints](#api-endpoints)
- [Project Structure](#project-structure)
- [Customization](#customization)
  - [Changing the Embedding Model](#changing-the-embedding-model)
  - [Provider Logic](#provider-logic)
- [Data Persistence](#data-persistence)

## Features
- **Flexible Document Ingestion**: Supports PDF, DOCX, TXT, and Markdown file formats.
- **Complete RAG Pipeline**: Implements an end-to-end RAG workflow: text extraction, chunking, embedding, storage, retrieval, and generation.
- **High-Performance Embeddings**: Uses `fastembed` with the efficient `BAAI/bge-small-en-v1.5` model by default.
- **Fast Vector Search**: Employs FAISS for rapid, in-memory similarity searches, with automatic persistence to disk.
- **Dual LLM Provider Strategy**: Prioritizes the high-speed Groq API (`Llama 3.1 8B`) and automatically falls back to Google Gemini (`gemini-1.5-flash`) in case of rate limits or configuration issues.
- **Modern Async Backend**: Built with FastAPI for high performance and an async-first design.
- **Simple RESTful API**: Provides clear and easy-to-use endpoints for ingesting documents and asking questions.

## Architecture

The API follows a standard Retrieval-Augmented Generation (RAG) pattern.

1.  **Ingest (`/ingest` endpoint)**:
    - A user uploads a document (e.g., `resume.pdf`, `job_description.docx`).
    - The server saves the file and extracts its text content.
    - The text is cleaned and split into smaller, overlapping chunks.
    - Each chunk is converted into a numerical vector (embedding) using the configured sentence-transformer model.
    - The embeddings and their associated metadata (filename, chunk text) are added to the FAISS vector store.
    - The updated vector store index and metadata are saved to the `backend/data/` directory.

2.  **Chat (`/chat` endpoint)**:
    - A user submits a question (query).
    - The query is converted into an embedding using the same model.
    - The FAISS vector store is searched to find the `k` most semantically similar document chunks from the knowledge base.
    - These retrieved chunks are formatted as "context" and combined with the original query into a single, comprehensive prompt.
    - The prompt is sent to the primary LLM provider (Groq). If the provider is unavailable or rate-limited, it automatically falls back to the secondary provider (Gemini).
    - The LLM generates an answer based *only* on the provided context.
    - The final answer is returned to the user.
  
## Getting Started

### Prerequisites

- Python 3.9+
- An API key from Groq.
- An API key from Google AI Studio for the Gemini fallback.

### Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/your-username/personalized-resume-assistant.git
    cd personalized-resume-assistant/backend
    ```

2.  **Create and activate a virtual environment:**
    ```bash
    python -m venv venv
    source venv/bin/activate
    # On Windows: venv\Scripts\activate
    ```

3.  **Install the required dependencies:**
    *(A `requirements.txt` should be created with the following content.)*
    ```bash
    pip install "fastapi[all]" python-dotenv fastembed faiss-cpu numpy pypdf docx2txt httpx google-generativeai
    ```
    > **Note**: `faiss-cpu` is recommended for general compatibility. If you have a CUDA-enabled GPU and the appropriate toolkits installed, you can use `faiss-gpu` for better performance.

### Configuration

1.  In the `backend/` directory, create a file named `.env`.

2.  Add your API keys and any custom configurations to this file. The `EMBED_DIM` must match the output dimension of your chosen `EMBED_MODEL_NAME`.

    > **Important**: You must replace the placeholder values (`gsk_...`, `AIza...`) with your own personal API keys. The application uses these keys to authenticate with the Groq and Google AI services and will not function without them. The code does not contain any pre-configured or shared API keys.

    ```env
    # backend/.env

    # --- Required ---
    # Get keys from https://console.groq.com/keys
    GROQ_API_KEY="gsk_..."

    # Get keys from https://aistudio.google.com/app/apikey
    GOOGLE_API_KEY="AIza..."

    # --- Optional ---
    # Model from HuggingFace compatible with fastembed.
    # See: https://qdrant.github.io/fastembed/examples/Supported_Models/
    EMBED_MODEL_NAME="BAAI/bge-small-en-v1.5"

    # Output dimension of the embedding model.
    # 384 for bge-small-en-v1.5
    # 768 for all-MiniLM-L6-v2
    EMBED_DIM="384"
    ```


## Usage

You can run the entire full-stack application (frontend and backend) with a single command. The `npm run all` script is designed to start both servers, but it needs your terminal to be aware of the Python environment first. You only need to do this once per terminal session.

**To start the application:**

1.  **Open a terminal at the project root.**

2.  **Activate the Python virtual environment:**
    This command points your terminal to the Python tools installed for the backend, which is necessary for `npm` to find `uvicorn`.
    ```bash
    source backend/venv/bin/activate
    # On Windows, use: backend\venv\Scripts\activate
    ```

3.  **Navigate to the frontend and start the servers:**
    The `npm run all` command will now be able to correctly launch both the backend and frontend.
    ```bash
    cd frontend
    npm install # You only need to do this the first time
    npm run all
    ```

Your terminal will now show logs from both servers, and the application will open in your browser at `http://localhost:5173`.

### API Endpoints

#### Health Check

- **Endpoint**: `GET /healthz`
- **Description**: A simple health check to confirm the server is running and to see the configured embedding dimension.
- **Example**:
  ```bash
  curl http://127.0.0.1:8000/healthz
  ```
- **Response**:
  ```json
  {
    "status": "ok",
    "embed_dim": 384
  }
  ```

#### Ingest a Document

- **Endpoint**: `POST /ingest`
- **Description**: Upload a document (`.pdf`, `.docx`, `.txt`, `.md`) to be processed and added to the vector store.
- **Example**:
  ```bash
  curl -X POST \
    -F "file=@/path/to/your/resume.pdf" \
    http://127.0.0.1:8000/ingest
  ```
- **Response**:
  ```json
  {
    "chunks": 15,
    "filename": "resume.pdf"
  }
  ```

#### Chat with Documents

- **Endpoint**: `POST /chat`
- **Description**: Ask a question about the ingested documents. The API will retrieve relevant context and generate an answer.
- **Body**:
  - `query` (str): The question you want to ask.
  - `k` (int, optional, default: 10): The number of document chunks to retrieve as context.
- **Example**:
  ```bash
  curl -X POST \
    -H "Content-Type: application/json" \
    -d '{"query": "What are my top 3 skills relevant for a Python developer role?", "k": 5}' \
    http://127.0.0.1:8000/chat
  ```
- **Response**:
  ```json
  {
    "answer": "Based on the context, your top 3 skills for a Python developer role are: 1. Developing and maintaining RESTful APIs using FastAPI and Django. 2. Experience with containerization technologies like Docker. 3. Proficiency in data analysis libraries such as Pandas and NumPy."
  }
  ```

## Project Structure

```
personalized-resume-assistant/
├── backend/
│   ├── data/                  # Stores persisted data (index, metadata, uploads)
│   │   ├── uploads/
│   │   ├── index.faiss        # FAISS vector index
│   │   └── meta.json          # Metadata for vectors
│   ├── models/
│   │   └── providers.py       # Logic for calling LLM APIs (Groq, Gemini)
│   ├── rag/
│   │   ├── ingest.py          # Document parsing and chunking
│   │   ├── retrieval.py       # Prompt assembly logic
│   │   └── store.py           # FAISS VectorStore implementation
│   ├── app.py                 # Main FastAPI application, endpoints
│   ├── deps.py                # Embedding model loader
│   ├── .env                   # (You create this) API keys and config
│   └── requirements.txt       # (You create this) Python dependencies
└── README.md                  # This file
```

## Customization

### Changing the Embedding Model

You can easily swap the embedding model by changing the environment variables in your `.env` file.

1.  **`EMBED_MODEL_NAME`**: Set this to any model supported by `fastembed`. See the list of supported models.
2.  **`EMBED_DIM`**: You **must** update this to match the output dimension of the new model.

**Important**: If you change the embedding model, you must delete the `backend/data` directory to clear the old vector store. The server will create a new, empty store with the correct dimension on its next run. You will need to re-ingest all your documents.

### Provider Logic

The application is hardcoded to use Groq as the primary provider and Gemini as the fallback. This logic is located in `backend/app.py` within the `/chat` endpoint. It's designed for resilience:

- It first tries Groq.
- If it receives a `429 Too Many Requests` error or a `ValueError` (e.g., missing API key), it logs a warning and automatically tries Gemini.
- If any other HTTP error occurs with Groq, or if the Gemini fallback also fails, a `502` or `503` error is returned to the client.

## Data Persistence

The vector store is designed to be persistent across server restarts.

- The FAISS index is saved to `backend/data/index.faiss`.
- The corresponding metadata is saved to `backend/data/meta.json`.
- Uploaded source documents are kept in `backend/data/uploads/`.

To **reset the entire knowledge base**, simply stop the server and delete the `backend/data` directory. The directory and a new, empty vector store will be recreated the next time you start the server and ingest a file.

